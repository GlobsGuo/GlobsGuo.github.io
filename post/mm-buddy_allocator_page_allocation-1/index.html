<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>mm-buddy_allocator分配页框(上) - Globs&#39; Catchall</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Globs Guo" /><meta name="description" content="slab系统调用buddy分配器分配所需要的内存页，作为slab使用。 和slab系统不同，buddy系统主要响应较大(至少为一个内存页)的内" />






<meta name="generator" content="Hugo 0.58.3 with theme even" />


<link rel="canonical" href="https://globsguo.github.io/post/mm-buddy_allocator_page_allocation-1/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="mm-buddy_allocator分配页框(上)" />
<meta property="og:description" content="slab系统调用buddy分配器分配所需要的内存页，作为slab使用。 和slab系统不同，buddy系统主要响应较大(至少为一个内存页)的内" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://globsguo.github.io/post/mm-buddy_allocator_page_allocation-1/" />
<meta property="article:published_time" content="2019-10-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-10-24T00:00:00+00:00" />
<meta itemprop="name" content="mm-buddy_allocator分配页框(上)">
<meta itemprop="description" content="slab系统调用buddy分配器分配所需要的内存页，作为slab使用。 和slab系统不同，buddy系统主要响应较大(至少为一个内存页)的内">


<meta itemprop="datePublished" content="2019-10-24T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-10-24T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="6661">



<meta itemprop="keywords" content="kernel-sources,memory,buddy-allocator,kmalloc,expand,get_page_from_freelist,__alloc_pages_nodemask,zlcache," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="mm-buddy_allocator分配页框(上)"/>
<meta name="twitter:description" content="slab系统调用buddy分配器分配所需要的内存页，作为slab使用。 和slab系统不同，buddy系统主要响应较大(至少为一个内存页)的内"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Globs&#39; Catchall</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Globs&#39; Catchall</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">mm-buddy_allocator分配页框(上)</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-10-24 </span>
        
          <span class="more-meta"> 6661 words </span>
          <span class="more-meta"> 14 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
<ul>
<li><a href="#1-kmalloc-large">1. kmalloc_large</a>
<ul>
<li><a href="#1-1-alloc-kmem-pages">1.1. alloc_kmem_pages</a></li>
<li><a href="#1-2-alloc-pages-current">1.2. alloc_pages_current</a></li>
</ul></li>
<li><a href="#2-alloc-pages-nodemask">2. __alloc_pages_nodemask</a></li>
<li><a href="#3-get-page-from-freelist">3. get_page_from_freelist</a>
<ul>
<li><a href="#3-1-cpuset-zone-allowed-softwall">3.1. cpuset_zone_allowed_softwall</a></li>
<li><a href="#3-2-zone-watermark-ok">3.2. zone_watermark_ok</a></li>
<li><a href="#3-3-buffered-rmqueue">3.3. buffered_rmqueue</a>
<ul>
<li><a href="#3-3-1-rmqueue-bulk">3.3.1. rmqueue_bulk</a></li>
<li><a href="#3-3-2-rmqueue">3.3.2. __rmqueue</a>
<ul>
<li><a href="#3-3-2-1-rmqueue-smallest">3.3.2.1. __rmqueue_smallest</a></li>
<li><a href="#3-3-2-2-rmqueue-fallback">3.3.2.2. __rmqueue_fallback</a>
<ul>
<li><a href="#3-3-2-2-1-try-to-steal-freepages">3.3.2.2.1. try_to_steal_freepages</a></li>
<li><a href="#3-3-2-2-2-move-freepages">3.3.2.2.2. move_freepages</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#4-总结">4. 总结</a></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<p>slab系统调用buddy分配器分配所需要的内存页，作为slab使用。</p>

<p>和slab系统不同，buddy系统主要响应较大(至少为一个内存页)的内存分配请求，本文仍然从 <code>kmalloc</code> 函数的实现入手，结合伙伴系统的核心函数 <code>__alloc_pages_nodemask</code> ，说明伙伴系统分配页框的过程。</p>

<p>由于伙伴系统分配页框的流程十分复杂，本文只介绍分配页框的第一次尝试，即 <code>__alloc_pages_nodemask</code> 函数第一次调用 <code>get_page_from_freelist</code> 的具体流程； <code>__alloc_pages_slowpath</code> 函数放在下一篇文章。</p>

<h1 id="1-kmalloc-large">1. kmalloc_large</h1>

<p><code>kmalloc</code> 函数执行时，如果请求的内存大小大于 <strong>KMALLOC_MAX_CACHE_SIZE</strong> ，就会调用 <code>kmalloc_large</code> 函数完成内存分配操作。</p>

<p><code>kmalloc_large</code> 函数先根据请求的内存的大小调用 <code>get_order</code> 函数得出请求的内存的大小对应内存页的数量 <strong>order</strong> ，然后调用 <code>kmalloc_order</code> ：</p>

<pre><code class="language-C">void *kmalloc_order(size_t size, gfp_t flags, unsigned int order)
{
    void *ret;
    struct page *page;

    /*
     * 添加复合页面元数据标志，即释放内存时需要的信息。
     * 复合页面的一个应用是实现transhugepage，即透明的
     * hugepage。
     * 复合页面用连续的多个页面组成一个hugepage，其中的
     * 第一个页面称作head page，其余的页面成为tail page。
     * head page中保存着hugepage的元数据。 
	 */
    flags |= __GFP_COMP;
    page = alloc_kmem_pages(flags, order);
    ret = page ? page_address(page) : NULL;
    kmemleak_alloc(ret, size, 1, flags);
    return ret;
}
EXPORT_SYMBOL(kmalloc_order);
</code></pre>

<h2 id="1-1-alloc-kmem-pages">1.1. alloc_kmem_pages</h2>

<p>内核里这个函数的注释写道： <code>alloc_keme_pages</code> 会将新分配的页charge到kmem资源计数器。</p>

<p>这里的charge，即cgroup中的charge，结合charge字面的意思充电，可以理解为分配；对应的，uncharge可以理解为释放。</p>

<pre><code class="language-C">/*
 * alloc_kmem_pages charges newly allocated pages to the kmem resource counter
 * of the current memory cgroup.
 *
 * It should be used when the caller would like to use kmalloc, but since the
 * allocation is large, it has to fall back to the page allocator.
 */
struct page *alloc_kmem_pages(gfp_t gfp_mask, unsigned int order)
{
    struct page *page;
    struct mem_cgroup *memcg = NULL;

    /* 判断memcg是否支持新的页面分配操作 */
    if (!memcg_kmem_newpage_charge(gfp_mask, &amp;memcg, order))
        return NULL;
    page = alloc_pages(gfp_mask, order);
    memcg_kmem_commit_charge(page, memcg, order);
    return page;
}
</code></pre>

<h2 id="1-2-alloc-pages-current">1.2. alloc_pages_current</h2>

<p>NUMA系统中， <code>alloc_pages</code> 函数最终调用 <em>mm/mempolicy.c</em> 中的 <code>alloc_pages_current</code> 完成页面分配的操作。</p>

<pre><code class="language-C">struct page *alloc_pages_current(gfp_t gfp, unsigned order)
{
    struct mempolicy *pol = get_task_policy(current);
    struct page *page;
    unsigned int cpuset_mems_cookie;

    if (!pol || in_interrupt() || (gfp &amp; __GFP_THISNODE))
        pol = &amp;default_policy;

retry_cpuset:
    cpuset_mems_cookie = read_mems_allowed_begin();

    /*
     * No reference counting needed for current-&gt;mempolicy
     * nor system default_policy
     */
    if (pol-&gt;mode == MPOL_INTERLEAVE)
        page = alloc_page_interleave(gfp, order, interleave_nodes(pol));
    else
        page = __alloc_pages_nodemask(gfp, order,
                policy_zonelist(gfp, pol, numa_node_id()),
                policy_nodemask(gfp, pol));

    if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))
        goto retry_cpuset;

    return page;
}
EXPORT_SYMBOL(alloc_pages_current);
</code></pre>

<p><code>alloc_pages_current</code> 根据当前进程绑定的内存策略，执行不同的函数路径。</p>

<p>一种是 <strong>MPOL_INTERLEAVE</strong> ，执行 <code>alloc_page_interleave</code> ，首先获取可用的zonelist，最终通过 <code>__alloc_pages_nodemask</code> 分配内存页。<br />
否则直接调用 <code>__alloc_pages_nodemask</code> 函数分配内存页，第一种情况下 <strong>nodemask</strong> 参数为空。</p>

<h1 id="2-alloc-pages-nodemask">2. __alloc_pages_nodemask</h1>

<p><code>__alloc_pages_nodemask</code> 函数即zoned buddy allocator，是伙伴系统的核心，执行内存页的分配操作。</p>

<pre><code class="language-C">struct page *__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
            struct zonelist *zonelist, nodemask_t *nodemask)
{
    /* 根据申请内存时提供的gfp标志获取zone类型 */
    enum zone_type high_zoneidx = gfp_zone(gfp_mask);
    struct zone *preferred_zone;
    struct zoneref *preferred_zoneref;
    struct page *page = NULL;
    /* 根据gfp标志得到页面迁移的类型 */
    int migratetype = allocflags_to_migratetype(gfp_mask);
    unsigned int cpuset_mems_cookie;
    int alloc_flags = ALLOC_WMARK_LOW|ALLOC_CPUSET|ALLOC_FAIR;
    int classzone_idx;
    /* 移除不支持的标志 */
    gfp_mask &amp;= gfp_allowed_mask;

    lockdep_trace_alloc(gfp_mask);

    might_sleep_if(gfp_mask &amp; __GFP_WAIT);
    /*
     根据mm/page_alloc.c中的变量fail_page_alloc判断
     当前的设置是否符合分配失败的情况 */
    if (should_fail_alloc_page(gfp_mask, order))
        return NULL;

    /*
     初始化过后，zonelist-&gt;_zonerefs中包含所有可用的zone，
     并且以NULL作为结束标志 */
    if (unlikely(!zonelist-&gt;_zonerefs-&gt;zone))
        return NULL;

retry_cpuset:
    cpuset_mems_cookie = read_mems_allowed_begin();

    /* The preferred zone is used for statistics later */
    preferred_zoneref = first_zones_zonelist(zonelist, high_zoneidx,
                nodemask ? : &amp;cpuset_current_mems_allowed,
                &amp;preferred_zone);
    if (!preferred_zone)
        goto out;
    classzone_idx = zonelist_zone_idx(preferred_zoneref);

#ifdef CONFIG_CMA
    if (allocflags_to_migratetype(gfp_mask) == MIGRATE_MOVABLE)
        alloc_flags |= ALLOC_CMA;
#endif
    /* First allocation attempt */
    page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, nodemask, order,
            zonelist, high_zoneidx, alloc_flags,
            preferred_zone, classzone_idx, migratetype);
    if (unlikely(!page)) {
        /*
         * Runtime PM, block IO and its error handling path
         * can deadlock because I/O on the device might not
         * complete.
         */
        gfp_mask = memalloc_noio_flags(gfp_mask);
        page = __alloc_pages_slowpath(gfp_mask, order,
                zonelist, high_zoneidx, nodemask,
                preferred_zone, classzone_idx, migratetype);
    }

    trace_mm_page_alloc(page, order, gfp_mask, migratetype);

out:
    /*
     * When updating a task's mems_allowed, it is possible to race with
     * parallel threads in such a way that an allocation can fail while
     * the mask is being updated. If a page allocation is about to fail,
     * check if the cpuset changed during allocation and if so, retry.
     */
    if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))
        goto retry_cpuset;

    return page;
}
EXPORT_SYMBOL(__alloc_pages_nodemask);
</code></pre>

<p><code>__alloc_pages_nodemask</code> 通过 <code>first_zones_zonelist</code> 获取可以使用的第一个zoneref，成功后执行进一步操作—— <code>get_page_from_freelist</code> 。<br />
如果分配失败，移除可能存在的 <strong>PF_MEMALLOC_NOIO</strong> 标志，然后调用 <code>__alloc_pages_slowpath</code> 函数，再次尝试分配内存页。</p>

<h1 id="3-get-page-from-freelist">3. get_page_from_freelist</h1>

<p><code>get_page_from_freelist</code> 是 <code>__alloc_pages_nodemask</code> 分配页面的第一次尝试：</p>

<pre><code class="language-C">/*
 __alloc_pages_nodemask调用这个函数时，传入的参数
 只有gfp_mask发生了变化，增加了__GFP_HARDWALL标志，
 执行cpuset检查时会用到 */
static struct page *
get_page_from_freelist(gfp_t gfp_mask, nodemask_t *nodemask, unsigned int order,
        struct zonelist *zonelist, int high_zoneidx, int alloc_flags,
        struct zone *preferred_zone, int classzone_idx, int migratetype)
{
    struct zoneref *z;
    struct page *page = NULL;
    struct zone *zone;
    nodemask_t *allowednodes = NULL;
    int zlc_active = 0;
    int did_zlc_setup = 0;
    bool consider_zone_dirty = (alloc_flags &amp; ALLOC_WMARK_LOW) &amp;&amp;
                (gfp_mask &amp; __GFP_WRITE);
    int nr_fair_skipped = 0;
    bool zonelist_rescan;

zonelist_scan:
    zonelist_rescan = false;

    /* 扫描所有类型小于high_zoneidx的zone */
    for_each_zone_zonelist_nodemask(zone, z, zonelist,
                        high_zoneidx, nodemask) {
        unsigned long mark;

        /*
         如果zlcache已经生效，只有bitmap中没有标记为满，
         并且区域所属的节点包含在allowednodes中的zone
         值得一试 */
        if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; zlc_active &amp;&amp;
            !zlc_zone_worth_trying(zonelist, z, allowednodes))
                continue;
        /*
         开启了cpuset功能，ALLOC_CPUSET指明分配操作要考虑
         cpuset的限制，通过cpuset_zone_allowed_softwall
         判断当前的zone能否用于分配操作 */
        if (cpusets_enabled() &amp;&amp;
            (alloc_flags &amp; ALLOC_CPUSET) &amp;&amp;
            !cpuset_zone_allowed_softwall(zone, gfp_mask))
                continue;
        /*
         如果设置了ALLOC_FAIR，并且当前的zone和传入的
         preferred_zone不属于相同的节点，直接从当前的
         zone分配页面；否则增加由于fair被跳过的zone的
         计数 */
        if (alloc_flags &amp; ALLOC_FAIR) {
            if (!zone_local(preferred_zone, zone))
                break;
            if (zone_is_fair_depleted(zone)) {
                nr_fair_skipped++;
                continue;
            }
        }
        /* dirty页筛选，保证每个zone的dirty页数量在限制内 */
        if (consider_zone_dirty &amp;&amp; !zone_dirty_ok(zone))
            continue;

        mark = zone-&gt;watermark[alloc_flags &amp; ALLOC_WMARK_MASK];
        /*# watermark is not OK */
        if (!zone_watermark_ok(zone, order, mark,
                       classzone_idx, alloc_flags)) {
            int ret;

            /* Checked here to keep the fast path fast */
            BUILD_BUG_ON(ALLOC_NO_WATERMARKS &lt; NR_WMARK);
            /*# don't care about watermark */
            if (alloc_flags &amp; ALLOC_NO_WATERMARKS)
                goto try_this_zone;

            if (IS_ENABLED(CONFIG_NUMA) &amp;&amp;
                    !did_zlc_setup &amp;&amp; nr_online_nodes &gt; 1) {
                /*
                 建立zlcache，返回可用的节点。
                 如果距离上一次zap位图达到1s，执行zap操作 */
                allowednodes = zlc_setup(zonelist, alloc_flags);
                zlc_active = 1;
                did_zlc_setup = 1;
            }

            /*
             系统不允许zone回收，或者允许zone回收，但是当前
             zone距离perferred_zone距离过大，标记为full */
            if (zone_reclaim_mode == 0 ||
                !zone_allows_reclaim(preferred_zone, zone))
                goto this_zone_full;

            /*
             * As we may have just activated ZLC, check if the first
             * eligible zone has failed zone_reclaim recently.
             */
            if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; zlc_active &amp;&amp;
                !zlc_zone_worth_trying(zonelist, z, allowednodes))
                continue;

            /* 获取可回收的页面数 */
            ret = zone_reclaim(zone, gfp_mask, order);
            switch (ret) {
            case ZONE_RECLAIM_NOSCAN:
                /* did not scan */
                continue;
            case ZONE_RECLAIM_FULL:
                /* scanned but unreclaimable */
                continue;
            default:
                /* did we reclaim enough */
                if (zone_watermark_ok(zone, order, mark,
                        classzone_idx, alloc_flags))
                    goto try_this_zone;

                /*
                 * Failed to reclaim enough to meet watermark.
                 * Only mark the zone full if checking the min
                 * watermark or if we failed to reclaim just
                 * 1&lt;&lt;order pages or else the page allocator
                 * fastpath will prematurely mark zones full
                 * when the watermark is between the low and
                 * min watermarks.
                 */
                /*
                 如果已经使用的是最小的watermark，或者只能
                 回收部分页面，但是仍然超过了watermark，在
                 zlcache标记zone为full。
                 这个标记动作只是在zlcache， */
                if (((alloc_flags &amp; ALLOC_WMARK_MASK) == ALLOC_WMARK_MIN) ||
                    ret == ZONE_RECLAIM_SOME)
                    goto this_zone_full;

                continue;
            }
        }

try_this_zone:
        /*
         wartermark高于限制值，或者设置了ALLOC_NO_WATERMARKS，
         或者执行了回收操作后watermark高于限制值 */
        page = buffered_rmqueue(preferred_zone, zone, order,
                        gfp_mask, migratetype);
        if (page)
            break;
this_zone_full:
        if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; zlc_active)
            zlc_mark_zone_full(zonelist, z);
    }

    if (page) {
        page-&gt;pfmemalloc = !!(alloc_flags &amp; ALLOC_NO_WATERMARKS);
        return page;
    }
    if (alloc_flags &amp; ALLOC_FAIR) {
        alloc_flags &amp;= ~ALLOC_FAIR;
        if (nr_fair_skipped) {
            zonelist_rescan = true;
            reset_alloc_batches(preferred_zone);
        }
        if (nr_online_nodes &gt; 1)
            zonelist_rescan = true;
    }

    if (unlikely(IS_ENABLED(CONFIG_NUMA) &amp;&amp; zlc_active)) {
        /* Disable zlc cache for second zonelist scan */
        zlc_active = 0;
        zonelist_rescan = true;
    }

    if (zonelist_rescan)
        goto zonelist_scan;

    return NULL;
}
</code></pre>

<h2 id="3-1-cpuset-zone-allowed-softwall">3.1. cpuset_zone_allowed_softwall</h2>

<p>如果系统开启了cpuset功能， <code>cpuset_zone_allowed_softwall</code> 最终调用 <code>__cpuset_node_allowed_softwall</code> 实现cpuset的限制功能：</p>

<pre><code class="language-C">int __cpuset_node_allowed_softwall(int node, gfp_t gfp_mask)
{
    struct cpuset *cs;      /* current cpuset ancestors */
    int allowed;            /* is allocation in zone z allowed? */

    if (in_interrupt() || (gfp_mask &amp; __GFP_THISNODE))
        return 1;
    might_sleep_if(!(gfp_mask &amp; __GFP_HARDWALL));
    if (node_isset(node, current-&gt;mems_allowed))
        return 1;
    /*
     * Allow tasks that have access to memory reserves because they have
     * been OOM killed to get memory anywhere.
     */
    if (unlikely(test_thread_flag(TIF_MEMDIE)))
        return 1;
    if (gfp_mask &amp; __GFP_HARDWALL)  
        /* If hardwall request, stop here */
        return 0;

    if (current-&gt;flags &amp; PF_EXITING) /* Let dying task have memory */
        return 1;

    /* Not hardwall and node outside mems_allowed: scan up cpusets */
    mutex_lock(&amp;callback_mutex);

    rcu_read_lock();
    cs = nearest_hardwall_ancestor(task_cs(current));
    allowed = node_isset(node, cs-&gt;mems_allowed);
    rcu_read_unlock();

    mutex_unlock(&amp;callback_mutex);
    return allowed;
}
</code></pre>

<p>这里直接引用代码中的注释对函数进行说明：</p>

<blockquote>
<p>If we&rsquo;re in interrupt, yes, we can always allocate.  If __GFP_THISNODE is set, yes, we can always allocate.  If node is in our task&rsquo;s mems_allowed, yes.  If it&rsquo;s not a __GFP_HARDWALL request and this node is in the nearest hardwalled cpuset ancestor to this task&rsquo;s cpuset, yes.  If the task has been OOM killed and has access to memory reserves as specified by the TIF_MEMDIE flag, yes.<br />
Otherwise, no.</p>
</blockquote>

<p>处于中断状态；设置了 <strong>__GFP_THISNODE</strong> ；节点包含在 <strong>mems_allowed</strong> ，三种情况下可以直接允许分配。<br />
如果不是 <strong>__GFP_HARDWALL</strong> 请求，但是节点位于当前进程的cpuset的祖先cpuset内，允许分配。<br />
如果进程由于OOM被杀死，允许分配。<br />
其他情况不允许分配。</p>

<blockquote>
<p>If __GFP_HARDWALL is set, cpuset_node_allowed_softwall() reduces to cpuset_node_allowed_hardwall().  Otherwise, cpuset_node_allowed_softwall() might sleep, and might allow a node from an enclosing cpuset.<br />
cpuset_node_allowed_hardwall() only handles the simpler case of hardwall cpusets, and never sleeps.</p>
</blockquote>

<p>设置 <strong>__GFP_HARDWALL</strong> 函数直接缩减为 <code>cpuset_node_allowed_hardwall</code> ；不设置的话， <code>cpuset_node_allowed_software</code> 可能休眠。</p>

<blockquote>
<p>The __GFP_THISNODE placement logic is really handled elsewhere, by forcibly using a zonelist starting at a specified node, and by (in get_page_from_freelist()) refusing to consider the zones for any node on the zonelist except the first.  By the time any such calls get to this routine, we should just shut up and say &lsquo;yes&rsquo;.</p>
</blockquote>

<p><strong>_GFP_THISNODE</strong> 标志在其他地方已经进行处理，因此直接返回允许。</p>

<blockquote>
<p>GFP_USER allocations are marked with the __GFP_HARDWALL bit, and do not allow allocations outside the current tasks cpuset unless the task has been OOM killed as is marked TIF_MEMDIE. GFP_KERNEL allocations are not so marked, so can escape to the nearest enclosing hardwalled ancestor cpuset.</p>
</blockquote>

<p><strong>GFP_USER</strong> 标志设置了 <strong>__GFP_HARDWALL</strong> ，不允许从当前进程的cpuset以外的zone分配内存，除非设置了 <strong>TIF_MEMDIE</strong> 标志。 <strong>GFP_KERNEL</strong> 没有这个标志，因此可以从最近的祖先cpuset分配内存。</p>

<blockquote>
<p>The first call here from mm/page_alloc:get_page_from_freelist() has __GFP_HARDWALL set in gfp_mask, enforcing hardwall cpusets, so no allocation on a node outside the cpuset is allowed (unless in interrupt, of course).</p>
</blockquote>

<p><em>mm/page_alloc.c</em> 中 <code>__alloc_pages_nodemask</code> 第一次调用 <code>get_page_from_freelist</code> 时，设置了 <strong>__GFP_HARDWALL</strong> 标志，不允许从cpuset外的节点分配内存(除非处于中断中)。</p>

<blockquote>
<p>The second pass through get_page_from_freelist() doesn&rsquo;t even call here for GFP_ATOMIC calls.  For those calls, the __alloc_pages() variable &lsquo;wait&rsquo; is not set, and the bit ALLOC_CPUSET is not set in alloc_flags.  That logic and the checks below have the combined affect that:</p>

<p>in_interrupt - any node ok (current task context irrelevant)<br />
GFP_ATOMIC   - any node ok<br />
TIF_MEMDIE   - any node ok<br />
GFP_KERNEL   - any node in enclosing hardwalled cpuset ok<br />
GFP_USER     - only nodes in current tasks mems allowed ok.</p>
</blockquote>

<p><code>__alloc_pages_nodemask</code> 执行慢路径 <code>__alloc_pages_slowpath</code> 时，还会调用 <code>get_page_from_freelist</code> 。如果设置了 <strong>GFP_ATOMIC</strong> ，不会执行这个函数。这些函数调用不会设置 <code>__alloc_pages</code> 的wait变量， <strong>alloc_flags</strong> 的 <strong>ALLOC_CPUSET</strong> 也不会设置。</p>

<h2 id="3-2-zone-watermark-ok">3.2. zone_watermark_ok</h2>

<p>对于列表中的每个zone，如果zlcache没有标记为满，并且位于可用的内存节点，符合cpuset的限制，并且没有设置 <strong>ALLOC_FAIR</strong> 分配标志，通过了dirty检查，则调用 <strong>zone_watermark_ok</strong> 函数判断当前zone的空闲页是否高于给定的watermark。</p>

<p><code>zone_watermark_ok</code> 函数通过 <code>__zone_watermark_ok</code> 函数实现：</p>

<pre><code class="language-C">static bool __zone_watermark_ok(struct zone *z, unsigned int order,
            unsigned long mark, int classzone_idx, int alloc_flags,
            long free_pages)
{
    /* free_pages my go negative - that's OK */
    long min = mark;
    int o;
    long free_cma = 0;

    free_pages -= (1 &lt;&lt; order) - 1;
    if (alloc_flags &amp; ALLOC_HIGH)
        min -= min / 2;
    if (alloc_flags &amp; ALLOC_HARDER)
        min -= min / 4;
#ifdef CONFIG_CMA    // x86默认未配置
    /* If allocation can't use CMA areas don't use free CMA pages */
    if (!(alloc_flags &amp; ALLOC_CMA))
        free_cma = zone_page_state(z, NR_FREE_CMA_PAGES);
#endif

    /*
     如果分配所需的页面后剩余的空闲页面的数量小于
     watermark + 每个zone的保留页的数量，返回false */
    if (free_pages - free_cma &lt;= min + z-&gt;lowmem_reserve[classzone_idx])
        return false;
    for (o = 0; o &lt; order; o++) {
        /* At the next order, this order's pages become unavailable */
        free_pages -= z-&gt;free_area[o].nr_free &lt;&lt; o;

        /* Require fewer higher order pages to be free */
        min &gt;&gt;= 1;

        if (free_pages &lt;= min)
            return false;
    }
    return true;
}
</code></pre>

<h2 id="3-3-buffered-rmqueue">3.3. buffered_rmqueue</h2>

<p>如果找到了可以使用的zone，就通过 <code>buffered_rmqueue</code> 从zone中分配所需的内存页：</p>

<pre><code class="language-C">static inline
struct page *buffered_rmqueue(struct zone *preferred_zone,
            struct zone *zone, unsigned int order,
            gfp_t gfp_flags, int migratetype)
{
    unsigned long flags;
    struct page *page;
    bool cold = ((gfp_flags &amp; __GFP_COLD) != 0);

again:
    if (likely(order == 0)) {
        struct per_cpu_pages *pcp;
        struct list_head *list;

        local_irq_save(flags);
        pcp = &amp;this_cpu_ptr(zone-&gt;pageset)-&gt;pcp;
        /* 获取当前CPU给定migratetype的page表 */
        list = &amp;pcp-&gt;lists[migratetype];
        /*
         per-cpu页框高速缓存为空，调用rmqueue_bulk从
         伙伴系统申请batch个内存页进行补充 */
        if (list_empty(list)) {
            pcp-&gt;count += rmqueue_bulk(zone, 0,
                    pcp-&gt;batch, list,
                    migratetype, cold);
            if (unlikely(list_empty(list)))
                goto failed;
        }

        /* 从list头部取hot page，从尾部取cold page */
        if (cold)
            page = list_entry(list-&gt;prev, struct page, lru);
        else
            page = list_entry(list-&gt;next, struct page, lru);

        list_del(&amp;page-&gt;lru);
        pcp-&gt;count--;
    } else {
        if (unlikely(gfp_flags &amp; __GFP_NOFAIL)) {
            /* 警告大于2个page的nofail申请 */
            WARN_ON_ONCE(order &gt; 1);
        }
        spin_lock_irqsave(&amp;zone-&gt;lock, flags);
        /* 多页请求通过伙伴系统申请 */
        page = __rmqueue(zone, order, migratetype);
        spin_unlock(&amp;zone-&gt;lock);
        if (!page)
            goto failed;
        /* 减少zone的空闲页面计数 */
        __mod_zone_freepage_state(zone, -(1 &lt;&lt; order),
                      get_freepage_migratetype(page));
    }

    /* 减去zone的NR_ALLOC_BATCH计数 */
    __mod_zone_page_state(zone, NR_ALLOC_BATCH, -(1 &lt;&lt; order));
    /* 将NR_ALLOC_BATCH为0的zone也标记为fair depleted */
    if (zone_page_state(zone, NR_ALLOC_BATCH) == 0 &amp;&amp;
        !zone_is_fair_depleted(zone))
        zone_set_flag(zone, ZONE_FAIR_DEPLETED);

    /* 增加vm_event计数 */
    __count_zone_vm_events(PGALLOC, zone, 1 &lt;&lt; order);
    zone_statistics(preferred_zone, zone, gfp_flags);
    local_irq_restore(flags);

    VM_BUG_ON_PAGE(bad_range(zone, page), page);
    /* 根据分配标志符初始化申请到的内存页，下列原因会
     导致检查失败：
     1. _mapcount != 0
     2. mapping != NULL
     3. _count != 0
     4. 一些flag不为0
     5. cgroup检查失败  */
    if (prep_new_page(page, order, gfp_flags))
        goto again;
    return page;

failed:
    local_irq_restore(flags);
    return NULL;
}
</code></pre>

<h3 id="3-3-1-rmqueue-bulk">3.3.1. rmqueue_bulk</h3>

<p>作为补充per-cpu页框高速缓存的内存页的函数， <code>rmqueue_bulk</code> 通过 <code>__rmqueue</code> 依次从伙伴系统申请 <strong>batch</strong> 个页框，并根据传入的 <strong>cold</strong> 参数将申请到的页面依次添加到传入的 <strong>list</strong> (即per-cpu页框高速缓存的页框链表)的头部(cold为false)或者尾部(cold为true)。</p>

<p>函数会修改zone的空闲页面统计数，返回真正申请到的页面数。</p>

<h3 id="3-3-2-rmqueue">3.3.2. __rmqueue</h3>

<p><code>__rmqueue</code> 是“真正”的伙伴系统分配函数，函数第一次尝试分配请求的migratetype所需的页面数；如果失败，并且第一次请求的migratetype不是 <strong>MIGRATE_RESERVE</strong> ，再调用 <code>__rmqueue_fallback</code> 尝试分配；如果分配再次失败，则将migratetype置为 <strong>MIGRATE_RESERVE</strong> ，从保留内存区域进行分配。</p>

<pre><code class="language-C">static struct page *__rmqueue(struct zone *zone, unsigned int order,
                        int migratetype)
{
    struct page *page;

retry_reserve:
    page = __rmqueue_smallest(zone, order, migratetype);

    if (unlikely(!page) &amp;&amp; migratetype != MIGRATE_RESERVE) {
        page = __rmqueue_fallback(zone, order, migratetype);

        /*
         * Use MIGRATE_RESERVE rather than fail an allocation. goto
         * is used because __rmqueue_smallest is an inline function
         * and we want just one call site
         */
        if (!page) {
            migratetype = MIGRATE_RESERVE;
            goto retry_reserve;
        }
    }

    trace_mm_page_alloc_zone_locked(page, order, migratetype);
    return page;
}
</code></pre>

<h4 id="3-3-2-1-rmqueue-smallest">3.3.2.1. __rmqueue_smallest</h4>

<p><code>__rmqueue_smallest</code> 是 <code>__rmqueue</code> 函数分配页框的第一次尝试。<br />
函数从传入的order开始，从zone中最接近order的 <strong>free_area</strong> 中分配页框。</p>

<pre><code class="language-C">static inline
struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
                        int migratetype)
{
    unsigned int current_order;
    struct free_area *area;
    struct page *page;

    /* Find a page of the appropriate size in the preferred list */
    for (current_order = order; current_order &lt; MAX_ORDER; ++current_order) {
        area = &amp;(zone-&gt;free_area[current_order]);
        if (list_empty(&amp;area-&gt;free_list[migratetype]))
            continue;

        page = list_entry(area-&gt;free_list[migratetype].next,
                            struct page, lru);
        list_del(&amp;page-&gt;lru);
        rmv_page_order(page);
        area-&gt;nr_free--;
        expand(zone, page, order, current_order, area, migratetype);
        set_freepage_migratetype(page, migratetype);
        return page;
    }

    return NULL;
}
</code></pre>

<p>如果没有和给定的order相同的 <strong>free_area</strong> ，<code>__rmqueue_smallest</code> 从最接近order的 <strong>free_area</strong> 分配内存，然后 调用<code>expand</code> 函数将原来order较大的内存块切分成较小的内存块，并且添加到相应的链表中。</p>

<h4 id="3-3-2-2-rmqueue-fallback">3.3.2.2. __rmqueue_fallback</h4>

<p>如果 <code>__rmqueue_smallest</code> 分配失败，并且migratetype不是reserve类型，则调用 <code>__rmqueue_fallback</code> 函数尝试分配。<br />
和 <code>__rmqueue_smallest</code> 不同， <code>__rmqueue_fallback</code> 从最大的order开始，从 <strong>fallback</strong> 表中，查找当前migratetype对应的可用migratetype，获取可用的 <strong>free_area</strong> ，然后从中分配页框。</p>

<pre><code class="language-C">static inline struct page *
__rmqueue_fallback(struct zone *zone, unsigned int order, int start_migratetype)
{
    struct free_area *area;
    unsigned int current_order;
    struct page *page;
    int migratetype, new_type, i;

    /* Find the largest possible block of pages in the other list */
    for (current_order = MAX_ORDER-1;
                current_order &gt;= order &amp;&amp; current_order &lt;= MAX_ORDER-1;
                --current_order) {
        for (i = 0;; i++) {
            migratetype = fallbacks[start_migratetype][i];

            /* MIGRATE_RESERVE handled later if necessary */
            /* MIGRATE_RESERVE是最后一个元素 */
            if (migratetype == MIGRATE_RESERVE)
                break;

            area = &amp;(zone-&gt;free_area[current_order]);
            if (list_empty(&amp;area-&gt;free_list[migratetype]))
                continue;

            page = list_entry(area-&gt;free_list[migratetype].next,
                    struct page, lru);
            area-&gt;nr_free--;
            /* start_migratetype是首选，migratetype是后备选项 */
            new_type = try_to_steal_freepages(zone, page,
                              start_migratetype,
                              migratetype);

            /* Remove the page from the freelists */
            list_del(&amp;page-&gt;lru);
            rmv_page_order(page);

            /*
             expand函数将
             free_area[current_order].freelist[new_type]
             切分成较小order的内存块，保存到响应列表 */
            expand(zone, page, order, current_order, area,
                   new_type);
            /* The freepage_migratetype may differ from pageblock's
             * migratetype depending on the decisions in
             * try_to_steal_freepages. This is OK as long as it does
             * not differ for MIGRATE_CMA type.
             */
            set_freepage_migratetype(page, new_type);

            trace_mm_page_alloc_extfrag(page, order, current_order,
                start_migratetype, migratetype, new_type);

            return page;
        }
    }

    return NULL;
}
</code></pre>

<p><strong>fallbacks</strong> 是定义在 <em>mm/page_alloc.c</em> 中的二维数组，保存着每种migratetype分配失败时，可以使用的其他migratetype。</p>

<p>如果 <code>__rmqueue_fallback</code> 也分配失败， <code>__rmqueue</code> 将migratetype设置成reserve类型，再次执行上面两个函数进行分配。</p>

<h5 id="3-3-2-2-1-try-to-steal-freepages">3.3.2.2.1. try_to_steal_freepages</h5>

<p>找到可用的 <strong>free_area</strong> 之后， <code>__rmqueue_fallback</code> 调用 <code>try_to_steal_freepages</code> ，根据 <strong>fallback_type</strong> 移动空闲页框。</p>

<p>内核中函数的注释为：</p>

<blockquote>
<p>切分一个大块的内存时，将所有空闲页框移动到首选的分配列表。如果fallback_type是可回收的内核分配，更积极主动的获取空闲页面的所有权。</p>

<p>另一方面，不要改变 <strong>MIGRATE_CMA</strong> 内存块的migratetype，也不要将CMA的页框移动到其他的空闲列表中，我们不想从 <strong>MIGRATE_CMA</strong> 区域分配不可用的页框。</p>
</blockquote>

<p>函数中的 <strong>pageblock_order</strong> 变量定义在 <em>include/linux/pageblock-flags.h</em> ，如果开启了 <strong>CONFIG_HUGETLB_PAGE</strong> (默认开启)，并且没有通过内核配置提供order值，就采用 <strong>HUGETLB_PAGE_ORDER</strong> ，即9；否则为 <strong>MAX_ORDER-1</strong> ，即10。<br />
<strong>page_group_by_mobility_disabled</strong> 变量在 <code>build_all_zonelists</code> 设置，如果系统中页框数量太低，将其置为1，关闭“group by mobility”。</p>

<pre><code class="language-C">/*
 @start_type为首选的迁移类型，即请求内存时的类型
 @fallback_type为备选的类型，即通过fallbacks获取的类型 */
static int try_to_steal_freepages(struct zone *zone, struct page *page,
                  int start_type, int fallback_type)
{
    /* current_order为当前找到的内存块的阶数 */
    int current_order = page_order(page);

    /*
     * When borrowing from MIGRATE_CMA, we need to release the excess
     * buddy pages to CMA itself. We also ensure the freepage_migratetype
     * is set to CMA so it is returned to the correct freelist in case
     * the page ends up being not actually allocated from the pcp lists.
     */
    if (is_migrate_cma(fallback_type))
        return fallback_type;

    /* Take ownership for orders &gt;= pageblock_order */
    if (current_order &gt;= pageblock_order) {
        /*
         change_pageblock_range将找到的内存块按照
         pageblock_order的大小分割，都设置migratetype
         为start_type，即首选的迁移类型 */
        change_pageblock_range(page, current_order, start_type);
        return start_type;
    }

    /*
     下列情况还会将页框移动到，即更积极的获取空闲页框的
     所有权：
     1. 当前order大于等于pageblock_order的一半
     2. 首选为可回收的迁移类型
     3. 关闭了“group by mobility” */
    if (current_order &gt;= pageblock_order / 2 ||
        start_type == MIGRATE_RECLAIMABLE ||
        page_group_by_mobility_disabled) {
        int pages;

        /* 将内存块移动到start_type的列表 */
        pages = move_freepages_block(zone, page, start_type);

        /* Claim the whole block if over half of it is free */
        if (pages &gt;= (1 &lt;&lt; (pageblock_order-1)) ||
                page_group_by_mobility_disabled) {

            /*
             move_freepages中设置migratetype的操作，可能没有
             设置page的迁移类型——对齐到pageblock_nr_pages */
            set_pageblock_migratetype(page, start_type);
            return start_type;
        }

    }

    return fallback_type;
}
</code></pre>

<p>从代码可以看出，对于内存块阶数大于 <strong>pageblock_order</strong> 的情况，只是修改页框的migratetype，并没有真正移动空闲页；而“更积极的获取空闲页框的所有权”则意味着将空闲页框移动到指定的迁移类型的列表中。</p>

<h5 id="3-3-2-2-2-move-freepages">3.3.2.2.2. move_freepages</h5>

<p><code>try_to_steal_freepages</code> 调用 <code>move_freepages_block</code> 移动空闲页框，后者先获取内存块的起止物理页框号对应的 <strong>struct page</strong> ，并进行合法性检查(这里有个疑问，不知道为什么将起止pfn对齐到pageblock_nr_pages)，然后调用 <code>move_freepages</code> 函数，移动页框到参数指定的迁移类型。</p>

<pre><code class="language-C">int move_freepages(struct zone *zone,
              struct page *start_page, struct page *end_page,
              int migratetype)
{
    struct page *page;
    unsigned long order;
    int pages_moved = 0;

#ifndef CONFIG_HOLES_IN_ZONE
    /*
     * page_zone is not safe to call in this context when
     * CONFIG_HOLES_IN_ZONE is set. This bug check is probably redundant
     * anyway as we check zone boundaries in move_freepages_block().
     * Remove at a later date when no bug reports exist related to
     * grouping pages by mobility
     */
    BUG_ON(page_zone(start_page) != page_zone(end_page));
#endif

    for (page = start_page; page &lt;= end_page;) {
        /* Make sure we are not inadvertently changing nodes */
        VM_BUG_ON_PAGE(page_to_nid(page) != zone_to_nid(zone), page);

        /*
         这个函数有两个调用者，一个是正在说明的伙伴系统的
         move_freepages_block，这种情况能够通过下列两项
         检查；另一个是mm/page_isolation.c中的函数，可能
         不会通过检查？ */
        if (!pfn_valid_within(page_to_pfn(page))) {
            page++;
            continue;
        }

        if (!PageBuddy(page)) {
            page++;
            continue;
        }

        order = page_order(page);
        /* 移动内存块到相同阶数、传入的迁移类型 */
        list_move(&amp;page-&gt;lru,
              &amp;zone-&gt;free_area[order].free_list[migratetype]);
        /* 修改迁移类型 */
        set_freepage_migratetype(page, migratetype);

        /*
         如果page没有通过上面两项检查，就会导致 1&lt;&lt;order
         不等于 end_pfn - start_pfn */
        page += 1 &lt;&lt; order;
        pages_moved += 1 &lt;&lt; order;
    }

    return pages_moved;
}
</code></pre>

<p>需要说明的是， <code>move_freepages</code> 每次都会获取page的order，order有多种可能值。</p>

<h1 id="4-总结">4. 总结</h1>

<p><code>__alloc_pages_nodemask</code> 函数首先调用 <code>get_page_from_freelist</code> ，尝试获取空闲页。<br />
为了保证函数的执行速度， <code>get_page_from_freelist</code> 会使用zlcache加快空闲页框的查找速度。</p>

<p><code>get_page_from_freelist</code> 会执行两次扫描，第一次扫描考虑所有的限制条件，包括cpuset，<strong>ALLOC_FAIR</strong> 标志，dirty限制，不考虑远程节点；第二次扫描会忽略 <strong>ALLOC_FAIR</strong> 标志，考虑远程节点。</p>

<p>对于zone中空闲页框数低于watermark的情况， <code>get_page_from_freelist</code> 调用 <code>zone_reclaim</code> ，尝试从zone中回收页框，回收流程在《mm-buddy_allocator页框回收》介绍。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Globs Guo</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2019-10-24
        
    </span>
  </p>
  
  
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">Reward</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/wechat.jpg">
        <span>wechat</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/alipay.jpg">
        <span>alipay</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/kernel-sources/">kernel-sources</a>
          <a href="/tags/memory/">memory</a>
          <a href="/tags/buddy-allocator/">buddy-allocator</a>
          <a href="/tags/kmalloc/">kmalloc</a>
          <a href="/tags/expand/">expand</a>
          <a href="/tags/get_page_from_freelist/">get_page_from_freelist</a>
          <a href="/tags/__alloc_pages_nodemask/">__alloc_pages_nodemask</a>
          <a href="/tags/zlcache/">zlcache</a>
          </div>
      <nav class="post-nav">
        
        <a class="next" href="/post/mm-buddy_allocator_initialization/">
            <span class="next-text nav-default">mm-buddy_allocator初始化</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="GlobsGuo/utterancesRepo"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="sendtomedivh@126.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/globsguo" class="iconfont icon-github" title="github"></a>
  <a href="https://globsguo.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Globs Guo</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>








</body>
</html>
